{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera_in_ex_para import *\n",
    "from utlis import *\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(\"./smpl\")\n",
    "from smpl import SMPL,SMPL_MODEL_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = '../data/pub_datas.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = np.load(pkl_path,allow_pickle=True)\n",
    "data = datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'pc_data', 'description', 'smpl', 'smpl_op', 'index', 'livehps_map', 'livehps_pc'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This person use his left hand to brace the pillar , then swing his right leg forward to the right back , end with his left foot behind .',\n",
       " 'A person brace the pillar with his left hand , swing his right leg forward and place his left foot behind .',\n",
       " 'Person brace pillar with left hand , swung right leg forward , left foot behind .',\n",
       " 'The person rest his left hand on the pillar and stand on one foot to the left , then kick his right leg straight downward and sway it backward behind his body , slide his left hand forward on the pillar and bring his right foot to the left rear side of his left foot , lightly turn his body to the right .',\n",
       " 'A person rest one hand on a pillar and kick one leg back , slide his hand forward and turn his body right .',\n",
       " 'Person rest left hand on pillar , stand on one foot , kick right leg down , sway it back , slid left hand forward , bring right foot to left rear , lightly turn body right .']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['description'] # store the text annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 75)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['smpl_op'].shape # global R, T, pose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* obtain the body mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using a SMPL model, with only 10 shape coefficients.\n"
     ]
    }
   ],
   "source": [
    "smpl_model = SMPL(SMPL_MODEL_DIR, create_transl=False)\n",
    "shape_blob = torch.Tensor(np.loadtxt(\"./shape.txt\"))\n",
    "globalR,trans,pose_blob=np.array(data[\"smpl_op\"][:,:3]),np.array(data[\"smpl_op\"][:,3:6]),np.array(data[\"smpl_op\"][:,6:])\n",
    "output = smpl_model(betas=shape_blob.reshape(1,-1), \n",
    "                    body_pose=torch.Tensor(pose_blob),      # M*69 (pose[:,3:])\n",
    "                    global_orient=torch.Tensor(globalR),    # M*3  (pose[:,:3])\n",
    "                    transl=torch.Tensor(trans))             # M*3  global T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144, 5)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pc_data'] # store the sparse point cloud path,  length--M\n",
    "pc_path = '/'.join(['../data','pc_img_data',str(data['index']),data['pc_data'][0]])\n",
    "pc_data = np.fromfile(pc_path,dtype = np.float32).reshape(-1,5)\n",
    "pc_data.shape # (262144, 5), four dimension is x,y,z,r. this is for cropping the dynamic interaction person or dynamic environment input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
